% This is lnicst.tex the demonstration file of the LaTeX macro package for
% Lecture Notes of the Institute for Computer Sciences, Social-Informatics 
% and Telecommunications Engineering series from Springer-Verlag.
% It serves as a template for authors as well.
% version 1.0 for LaTeX2e
%
\documentclass[lnicst,a4paper]{svmultln}
%
\usepackage[english]{babel}
\selectlanguage{english}
%
\usepackage{svg}
%
\usepackage{gensymb}
%
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.svg,.jpg,.png,.eps} % first try pdf, then eps, png and jpg
%
\usepackage{makeidx}  % allows for indexgeneration
% \makeindex          % be prepared for an author index
%
\begin{document}
%
\mainmatter              % start of the contribution
%
\title{Humanoid robot obstacle recognition via data filtering, localisation and robot-to-robot communication in context of RoboCup} 
%
\titlerunning{Humanoid robot obstacle recognition}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Benjamin Scholz \and Daniel Speck \and Judith Hartfill}
%
\authorrunning{Benjamin Scholz \and Daniel Speck \and Judith Hartfill}   % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{}
%
\institute{University of Hamburg, Fachbereich Informatik\\
\email{2scholz@informatik.uni-hamburg.de, 2speck@informatik.uni-hamburg.de, 2hartfil@informatik.uni-hamburg.de}
}

\maketitle              % typeset the title of the contribution
% \index{Ekeland, Ivar} % entries for the author index
% \index{Temam, Roger}  % of the whole volume
% \index{Dean, Jeffrey}




%%%%%%%%%%%%%%%%
%%%          %%%
%%% Abstract %%%
%%%          %%%
%%%%%%%%%%%%%%%%

\begin{abstract}        % give a summary of your paper
The abstract should summarize the contents of the paper. It will be set in 9-point
font size and be inset 1.0 cm from the right and left margins.
There will be two blank lines before and after the Abstract.

Use the abstract section to provide a teaser for the contents of your report I Do not attempt to write a review or summary I Be concise: Your abstract should have 200 words or less (do
not use more than 250 words)
%                         please supply keywords within your abstract
\keywords {obstacle recognition, obstacle avoidance, localisation, filtering, data smoothing, vision, swarm intelligence}
\end{abstract}
%




%%%%%%%%%%%%%%%%%%%%
%%%              %%%
%%% Introduction %%%
%%%              %%%
%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
 
 
In this project our aim was to implement obstacle avoidance on for a team of robots, that takes part in the RoboCup Soccer Humanoid Kid Size League. Until then, each robot in that team only made decisions only due to its own information, like the vision field. Our idea was to build an intern team communication system, that provides one robots information for every team member. Thus a form of swarm intelligence could increase the quality of the robots acting together as a whole team by increasing the quality of information. Considering that, it was necessary to develop a world model to make the information transfomable, unique and easy to provide. Furthermore we detected, that the image prosessing quality was not good enough to precisly determine the robots position and so we did some vision modifications and added some filters. In the end we used all this to increase the robots reasoning and its behaviour/decision making. When people are playing soccer together it always is a form a communication. They share information to play better as a hole team. We tried to transfer this very important aspect of human soccer to humanoid robot soccer, too.


Outline of underlying concepts

Brief summary of relevant theoretical background knowledge

Review of existing (published) work relevant for your topic(s)

Motivate the reader for the issue(s) you are trying to solve

Explain why your work (your approach) is necessary




%%%%%%%%%%%%%%%%%%
%%%            %%%
%%% Motivation %%%
%%%            %%%
%%%%%%%%%%%%%%%%%%

\subsection{Motivation}
Obstacle recognition and dynamic behaviour towards obstacles is very important if several robots shall act together in a certain way. One of the main reasons for this is to avoid the physical contact between robots. As long as their hardware is not good enough to hold balance when beeing touched, they are very likely to fall and - in worst-case-scenario - bring about anothers robots downfall. This usually causes hight costs for hardware fixes and also disturbs the gameplay. Getting up again can take several seconds and in addition it is not within the meaning of fairplay toward the opposite robots. 
Besides, more and especially better knowledge about the surroundings is strategically important in the RoboCup competition. Having a reliably prospect of its own positions, the position of the ball and the opposites robots position combined with a good path finding algorithm can raise the number of scores significantly.
Both aspects cause a big increase in making the robot soccer play more realistic and authentic, which is the main aim of the RoboCup competions.
  was war das problem bisher?




%%%%%%%%%%%%%%%%
%%%          %%%
%%% Problems %%%
%%%          %%%
%%%%%%%%%%%%%%%%

\subsection{Problems}

\subsubsection{Vision}
For an intelligent obstacle recognition several sufficiently pre-processed data input is needed to localise the obstacle itself, localise the robot itself, classify the obstacle (ball, goal, teammate, enemy, other obstacle) and develop an adequate strategy of (re-)action.
\\
One of our first problems were the already implemented algorithm for obstacle detection and goal detection. We discovered that neither obstacles nor goals were tracked/recognized satisfactorily for our tasks.
\\
The obstacle detection was too imprecisely to fulfil our plans of an intelligent reaction and the biggest problem was the missing localisation of the robot itself which is realized by tracking the goals (via tracking the two goal posts). Even when the robot stands still completely in a distance of two metres directly in front of the goal the vision detects four to six goal posts most of the time.
\\
The majority of false positives were shadows of the real posts or random mirages/reflections on the walls. This resulted in "jumping" goal posts because for the behaviour two posts are selected and out of the four to six possible posts the selection process to distinguish between the possibilities was not stable.
\\
In addition to that even when the real goal posts could be identified for several frames the algorithm often was not able to calculate the correct relative distances to the goal posts.
\\
The algorithm was designed to draw a rectangle over a possible goal post but it has no specific filters for the dimensions. This leads to rectangles having a wrong position, height and width compared to the original post. Some recorded datasets showed rectangles with twice or half the size of the real post and positional failures of several 10 centimetres.
\\
Appropriately reacting behaviour systems e.g. for intelligent obstacle recognition, localisation and further path planning or even postprocessing filters would fail on such raw data.

\subsubsection{Filtering}
The whole topic of obstacle recognition relies on information which is extracted out of (heavily) noisy input data.
\\
For our solutions primarily the goal post data and the later calculated goal centre information is important to calculate real, absolute coordinates for localisation purposes on the field out of the relative camera input.
\\
The raw data of goal posts (relative tuples of $x, y$ coordinates) when recorded over several seconds (with a still standing robot) oftenly showed a spreaded distribution of points in a circle with a radius of nearly half a metre. Those datasets of 100 to 200 recorded samples had a standard deviation of 200 millimetres and higher.
\\
Having such noisy data renders absolute positioning tasks and intelligent obstacle recognition (avoidance in case of real obstacles, path planning in case of balls) impossible.
\\
Since the robots do not have auto-calibration features the calibration of the camera and motor angles deteriorate the noise problem. Once the calibration is manually done its configuration is static but the real values vary other time, for example when the robot falls accidently.
\\
In addition to that the manual calibration is expansive in terms of time.

\subsubsection{Localisation}
seeing an obsacle and seeing the goal does not help much increasign the behaviour, if there is no way to know were they are in relation to the robot. but this requires a system, that provides the robots own position. so we developed a world model, that fits our needs and posibilities and should also be expandable for longtime usage. A special problrm here was, that the soccer field, like in human soccer, too, is axial symmetric. this makes localisation even harder then in ususal mobile robot environments

\subsubsection{Communication}
When thinking about how to achieve most and best information for the robot, we thought about swarm intelligence mechanisms. Althought our Darwin-OP platfors do have a lot more computing capacity then usual swarm intelligent bots have, we still can benefit from researches in this part of robotics. particularly as communicating is a very important part in "real" human soccer.





%%%%%%%%%%%%%%%%%
%%%           %%%
%%% Solutions %%%
%%%           %%%
%%%%%%%%%%%%%%%%%

\section{Solutions}


\subsection{Hinweise (deleten!)}
What did you do and how did you do it?

Methods

Design

Implementation

Do not include every possible detail and avoid redundancy

Use subsections to emphasize certain aspects/components of
your work -
but do not overuse them!

Avoid the passive voice: Y was done by X, use the active voice: X did Y




%%%%%%%%%%%%%%
%%%        %%%
%%% Vision %%%
%%%        %%%
%%%%%%%%%%%%%%

\subsection{Vision}




%%%%%%%%%%%%%%%%%
%%%           %%%
%%% Filtering %%%
%%%           %%%
%%%%%%%%%%%%%%%%%

\subsection{Filtering}

\subsubsection{Data noise}
The majority of the bitbots code works without filters on raw data, especially in the case of goal information, which is needed to localise the robot on the field in our case, no data postprocessing exists.

\subsubsection{DBSCAN} Our approach to reduce the noise on data was implementing a two dimensional version of the DBSCAN (density-based scanning) algorithm \cite{ester:kriegel}, which we slightly customized to our needs.
\\
At first our code was an exact re-implementation in pure Python of the DBSCAN algorithm \cite{ester:kriegel}
originally proposed by Martin Ester et al. in 1996 but this procedure was just for testing to ensure the algorithm meets our requirements.
\\
Due to the performance issues of pure Python implementations we refactored the complete code and made use of Numpy (mainly) and other SciPy libraries. As a consequence of the C/C++ core implementations of those libraries the overall execution time of our code decreased.
\\
In addition to that the refactored, performant version of our DBSCAN is implemented object oriented. This leads to an overhead in space complexity but offered easier debugging as well as an additional improvement of execution time because the calculations are saved in objects and those cached results are re-used as much as possible.
\\
In our tested cases the pure Python implementation ran about 8 - 12 times slower on average (dependent on current background tasks) on our darwin-op hardware compared to our current object oriented Python implementation with Numpy and other SciPy libraries.
\\
Those optimizations are essential considering the very limited computing power of our current darwin-op robots.
\\
For calculating the goal information each goal posts raw input data (if post is seen) is recorded separately in buffers of a maximum size of 60 items on every new frame. If a buffer is full, the oldest entry is dropped and the newest saved. As data structure for the buffers we chose Python Deque objects.
\\
If the robot changes its position, old data sets would mess up the calculation, so our DBSCAN object automatically drops data sets which are older than 10 seconds to prevent those kind of failures. Minor positional changes in the meantime are no problems in most cases, because the DBSCAN would filter those datasets appropriately.
\\
In figure~\ref{fig:dbscan1} a graphical illustration of the DBSCAN algorithm
is shown.

\begin{figure}
 	\centerline{\includegraphics[width=0.7\textwidth]{dbscan1.pdf}}
	{\caption{DBSCAN graphical illustration.
	The points labeled with `C' are core points,
	the yellow ones labeled with `D' are density-reachable points
	and the red ones labeled with `N' noise points.
	The minimum points parameter is set to $4$ in this example.}\label{fig:dbscan1}}
\end{figure}

Throughout our testing process we experienced positive results with 6 minimum points and an epsilon of 50 as parameters for our DBSCAN implementation.




%%%%%%%%%%%%%%%%%%%%
%%%              %%%
%%% Localisation %%%
%%%              %%%
%%%%%%%%%%%%%%%%%%%%

\subsection{Localisation}

\subsubsection{Calculating our robot's position}
We didn't have a lot of data to work with. The only data that we could use were the distances to the goal and to the goalposts. There is a lot of other data: like already mentioned the distance to obstacles or other players on the field and the distance to the ball. But the problem with those is that they are not a fixed point on the field. They will change their position and that means they are not suitable to calculate our own position. 
\\
The data we actually have is not the direct distance but only the distance in front of the robot and the distance to the left of the robot. We will call the distance in front of the robot u and to the left of him v to stay consistent with the terms used in the code. figure~\ref{fig:uvvalues} contains an example of these values.
\begin{figure}
 	\centerline{\includegraphics[width=0.7\textwidth]{uvvalues.pdf}}
	{\caption{The grid world divides the field into a certain amount of cells.}\label{fig:uvvalues}}
\end{figure}
\\
At first we want to outline how we basically calculated the position of our own robots and afterwards we are going to explain all steps in detail.\\
For this calculation we used the u- and v-values to two goal posts. With these values we can calculate the direct distances to the goal and we get two lines that intersect exactly at the robot's location. So what we need to do is calculate the slope of the two lines so that we can form equations for them. Once we have these equations we can calculate the point of intersection and we will know the robot's position on the field.
\\
We are going to call the u- and v-values for the left post \(u_{1}\) and \(v_{1}\), and for the right post \(u_{2}\) and \(v_{2}\). With the u- and v-values it's easy to calculate the direct distance to the goal. You can use Pythagora's theorem to do that: 
\begin{equation}
	d = \sqrt{u^2+v^2}
\end{equation}
We do that for both goal posts and we are going to call the distance to the left post \(d_{1}\) and to the right post \(d_{2}\). Because we want to know the angle between these lines and the goal width we need to know this value as well. The goal width doesn't change and we can retrieve it in the code, but the robot's goal post recognition can be very inaccurate. Meaning even though in reallity the goal width is always the same, according to the robot's values the goal width is changing all the time. So the robot calculates it's own value for the goal width too. We use the u- and v-values to create vectors and calculate a vector between the two goal posts:
\begin{equation}
	\vec{gw}=\left(\begin{array}{c} u_{1} \\ v_{1} \end{array}\right) - \left(\begin{array}{c} u_{2} \\ v_{2} \end{array}\right)
\end{equation}
The absolute value of \(gw\) is the goal width.
\\
Now that we have these three values we can form a triangle like in figure~\ref{fig:triangle}.
\begin{figure}
 	\centerline{\includegraphics[width=0.6\textwidth]{triangle.pdf}}
	{\caption{}\label{fig:triangle}}
\end{figure}
\\
With the help of that triangle we are going to calculate the point of intersection of \(d_{1}\) and \(d_{2}\). Let's assume that we have a coordinate system and the point of origin is at the middle of the field. The reason for that is that the origin of our grid world, that is used to save the data, is in the middle of the field too. That means it's easier to translate the position found in that coordinate system to the grid world. But we will explain that in the next section. Otherwise it would have been easier to have the origin in the middle of the goal the robot is facing to. To calculate the point of intersection we need to know the slope of both lines. We calculate the two angles shown in figure~\ref{fig:triangle}.
The formula we used is the following:
\begin{equation}
	\alpha = \arccos{(\frac{a^2-b^2-c^2}{-2bc})}
\end{equation}
\\
Specifically these are the two formulas if you want to calculate the angle of the left post and the right post in the triangle:
\\
\begin{equation}
	angleleft = \arccos{\frac{(d_{1}^2 - d_{2}^2 - goalwidth^2)}{(-2*d_{2}*goalwidth)}}
	\label{equ:angleleft}
\end{equation}

\begin{equation}\
	angleright = \arccos{\frac{(d_{2}^2 - d_{1}^2 - goalwidth^2)}{(-2*d_{1}*goalwidth)}}
	\label{equ:angleright}
\end{equation}
\\
Now we know the two angles in the triangle. To convert the angles to the slopes in the coordinate system we can do the following:\\
\begin{equation}
	slopeleft = \frac{1}{\tan{angleleft}}
	\label{equ:slopeleft}
\end{equation}

On the right side we don't need the angle itself but 180 minus the angle:
\begin{equation}
	sloperight = \frac{1}{\tan{180 - angleright}}
	\label{equ:sloperight}
\end{equation}
Because we want to know the point of intersection of these two lines we need an equation for them:
\begin{equation}
	y_{1} = slopeleft * x + b_{1}
\end{equation}
\begin{equation}
	y_{2} = sloperight * x + b_{2}
\end{equation}
\(b_{1}\) and \(b_{2}\) are the points of intersection with the y-axis. Our coordinate system has it's point of origin at the middle point of the field. The reason for that is that the origin of our grid world is at that point as well. So converting the values is a lot easier. The unit of the point of intersection is now milimeter, but we convert these values to enter the robot's position into our grid world.
\\
\\
Now the robot knows it's own position. But it can't determine the opponent's position on the field yet, because it doesn't know which direction it faces. The robot has his own inner coordinate-system. 0,0 is the middle of the robot. The y-, and x-axis depend on the direction the robot faces.\\
So at first we calculate how many degrees the robot would have to turn to face one of the two goal posts. If the robot turns to that angle and would walk directly to the goal post he would end up directly in front of it, but not looking directly at it. To achieve that we need another angle. Now if the robot would turn that angle he would look directly at the goal post. So all we need to do at the end is, according to the exact situation at hand, either add or subtract the two angles we calculated and we would know the angle to the x-axis.
In figure~\ref{fig:angles} the first angle we mentioned is \(\beta_{2}\) and the second one is newangleright.
\begin{figure}
 	\centerline{\includegraphics[width=0.6\textwidth]{angles.pdf}}
	{\caption{Example of angles needed to calculate the angle to the x-axis of our grid}\label{fig:angles}}
\end{figure}
\(\beta_{1}\) and \(\beta_{2}\) are calculated with the following equation:
\begin{equation}
	\beta = \arcsin{(\frac{v}{d})}
\end{equation}
We explained earlier how we calculated angle\_right and angle\_left. angle\_left\_new and angle\_right\_new are just 90\degree minus the old angles. 
This only works if the old angle is under 90 degree, but this is always true for at least one of the two angles.
\\
Now there are different cases which affect how exactly we calculate the angle to the grid. At first we check if u1 or u2 is bigger. In our example we face the opponents goal. If u1 is bigger that means that the robot faces to the left and that the angle is bigger than 180\degree. In that case we have to add 180\degree and in the other case we have subtract from 180\degree. It also determines if we take angle\_right\_new or angle\_left\_new depending on the side the robot is facing. Meaning if u1 is bigger we use angle\_left\_new and if u2 is bigger we use angle\_right\_new.
\\
The v-values are also important. With these values we can determine if the triangle of the u- and v-values is inside or outside the triangle of the two goal posts.
So depending on that we need to either add beta and angle\_new or subtract beta from angle\_new. This all boils down to these lines in our code:
\\
lines grafik
\\
So now that we know the position of the robots and their orientation we can finally calculate the position of other robots on the field.

\subsubsection{Calculating other robot's positions}


%%%%%%%%%%%%%%%%%%
%%%            %%%
%%% Grid World %%%
%%%            %%%
%%%%%%%%%%%%%%%%%%

\subsubsection{Grid World}
Now that we know the positions of our own robots we need some way to share these positions, so that our team mates can use that knowledge and every robot in our team can profit. For that purpose we use the grid world. We needed a representation of the field, so we could store and use the data we generated. There are different approaches for that. Other approaches that we considered were a coordinate system that uses millimetres and tree based solution. In the end we decided to use said grid world. Even though Daniel had the idea for that representation of the field it turns out we are not the first robocup-team to use it. \cite{} So we need to divide the field into a certain number of cells. As shown in figure~\ref{fig:gridworld} the origin of the cells is at the midpoint of the field.
\begin{figure}
 	\centerline{\includegraphics[width=1\textwidth]{gridworld.pdf}}
	{\caption{The grid world divides the field into a certain amount of cells.}\label{fig:gridworld}}
\end{figure}
We had different iterations of the grid world with different origins. At first the origin cell was in the lower left corner. Then we decided it would be better if the origin cell were in the middle of our own goal, mostly because it made calculations needed to determine the position of the robots easier. But if we wanted to use these calculations for the opponents goal too, it is easier to have the origin cell in the middle. There is also a difference between choosing an even number and uneven number respectively on the x and the y direction. We decided to choose an uneven number for both, which means the origin cell is now in the middle of the centre spot. One benefit of that is that there is a cell that is in the middle of the goal where the goalie can stand. When we take the cell at position (0,0) for example and we compare it to a coordinate system, then the upper right edge of that cell would be at (0.5, 0.5) in the coordinate system. When we calculate positions we don't get integers but decimal digits. So if it is over 0.5 instead of the cell with the 0 position we will put it in the cell with the position 1. 
For that purpose we used a dictionary in Python. The indexes of the dictionary are the positions in the grid world. For example (0, 0) would be the center of the field. Then we use numbers to fill that dictionary. At initialization the dictionary will be filled with zeroes, which is representing that there is nothing at that particular position. Here is a quick overview of the numbers we fill the dictionary with and what they represent:\\
0: nothing\\
1: own goal\\
2: enemy goal\\
3: own position\\
4: team mate\\
5: opponent\\
6: unclassified obstacle\\
\\
So if we want to know the state of a certain position we call that dictionary with that position and get back a number between 0 and 6.
\\
Every robot can determine their own position themselves. The position of the team mates is shared over the wifi, which will be explained in the next section. The robots can determine the position of opponents that they see. They share those positions so everyone has the position of as many opponents as possible. The robots actually can't distinguish between team mate and opponent. But because we know all the positions of our team mates the robots can compare the position of the robots they see with the position of their team mates in their grid world. Only if there isn't a team mate in that cell they will identify the robot as an opponent. Another problem is to keep the map up-to-date. We send the whole map over the wifi, so it would be possible to check if there still is a robot on a certain position when the robot looks in that direction. Another solution would be to delete the positions of the opponents after a certain amount of time. There is the same problem with team mates. Currently we don't send which team mate shares the position, but that would be one solution. Whenever a team mate shares their new position a the receiving robot could delete the old position.
\\



%%%%%%%%%%%%%%%%%%%%%
%%%               %%%
%%% Communication %%%
%%%               %%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Communication}
Communication has always been one of the most important fields of resaerch in RoboCup, at least it was the may reason to found the instituition to find out whether two or more robots might posibilly be able to act together in a sensful way. 
Due to the increasing number of robots one team is allowed to have on the field ( up two eleven in 2050, as the roadmap plans) and the high costs for one robot it will become more and more necessary to cooperate with other teams. When there were the first tries of mixed teams some years ago, it was very fast obvious, that robots from different teams will have to communicate with each other.
So some years ago, a RoboCup Soccer Humanoid Kid Size team implemented a WiFi communication standart, the so calles MiteCom,  that is used by a lot of taems in this league and makes it very easy to to play in one team with robots from different teams.
This standart already was used in our team for very simple behaviour. for example because of the lack of localisation, it was not possile for a fiel player to determine whether a goal was the opposite goal or his own goal. So the goalie could send information like "The ball is very close to me" and thus make the field player stop playing on that goal, but turning around and trying the ohter goal. But this was a very unsecure mechanism, because the WiFi on competitions has a very high package loss and is very unreliable. 
So our idea was, that every robot could provide its own gridworld and in return receive every other robots gridworlds. By weighting the incoming data due to reliability (like taking the median for each position of ten or so incoming grids) or actuality (I did not see a goal for a long time, dont know whether my own position is right or not, i will take a new grid and search a goal) the average information quantity and quality could increase a lot.
But our a big problem was, that via MiteCom it is only possible to send 32 bit integer values. Foremost we thought about implementing our own standart, but when thinking about it a secound time, it seemed a good idea to use it. First of all, because of the aspect of universality mentioned above, that will be needed in the future. But forthermore it is not a bad idea to only send integers. 

nur ein int
codierung, mehrere für werte und feldteile

%%%%%%%%%%%%%%%
%%%         %%%
%%% Results %%%
%%%         %%%
%%%%%%%%%%%%%%%

\section{Results}

Present your results in a logical sequence

Highlight the importance of your results and explain your
analysis methodology

Discuss the results you infer from your work

Important:
Adopt a critical approach in your discussion

Do not oversell your results - put the advantages first, but
don’t forget to mention the shortcomings!




%%%%%%%%%%%%%%%
%%%         %%%
%%% Summary %%%
%%%         %%%
%%%%%%%%%%%%%%%

\section{Summary}

Be more informative than your abstract!

Include a concise version of your discussion

Highlight what you found out

Highlight the problems you encountered

Explain how your results support your conclusions!

Provide suggestions for future research and briefly outline how
suggested research can be attempted

Important:
Make this section readable




%%%%%%%%%%%%%%%%%%
%%%            %%%
%%% References %%%
%%%            %%%
%%%%%%%%%%%%%%%%%%

\section{References}
Very important section of your report

If you used external information/results
)
Provide a
reference!

References will help the reader understand the basis of your
work and provide context for comparison

Use of references might also help you to be more concise

There are several types of reference

Book

Journal article

Conference publication

Web site

Web sites are usually unchecked sources -
be careful




%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                    %%%
%%% Notes and Comments %%%
%%%                    %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Notes and Comments.}
The first results on subharmonics were
obtained by Foster and Kesselman in \cite{fos:kes}, who showed the existence of
infinitely many subharmonics both in the subquadratic and superquadratic
case, with suitable growth conditions on $H'$. Again the duality
approach enabled Foster and Waterman in \cite{fos:kes:2} to treat the
same problem in the convex-subquadratic case, with growth conditions on
$H$ only.

Recently, Smith and Waterman (see \cite{smit:wat} and May et al. \cite{mes})
have obtained lower bound on the number of subharmonics of period $kT$,
based on symmetry considerations and on pinching estimates, as in
Sect.~5.2 of this article.




%%%%%%%%%%%%%%%%%%%%
%%%              %%%
%%% Bibliography %%%
%%%              %%%
%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{5}

\bibitem{ester:kriegel} Ester, M., Kriegel, H.-P., Sander, J., Xu, X.:
A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
In: Proceedings of 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96),
pp. 226--231. AAAI Press, Munich (1996)

\bibitem{smit:wat} Smith, T.F., Waterman, M.S.: Identification of Common Molecular
Subsequences. J. Mol. Biol. 147, 195--197 (1981)

\bibitem{mes} May, P., Ehrlich, H.C., Steinke, T.: ZIB Structure Prediction Pipeline:
Composing a Complex Biological Workflow through Web Services. In: Nagel,
W.E., Walter, W.V., Lehner, W. (eds.) Euro-Par 2006. LNCS, vol. 4128,
pp. 1148--1158. Springer, Heidelberg (2006)

\bibitem{fos:kes} Foster, I., Kesselman, C.: The Grid: Blueprint for a New Computing
Infrastructure. Morgan Kaufmann, San Francisco (1999)

\bibitem{cff} Czajkowski, K., Fitzgerald, S., Foster, I., Kesselman, C.: Grid
Information Services for Distributed Resource Sharing. In: 10th IEEE
International Symposium on High Performance Distributed Computing, pp.
181--184. IEEE Press, New York (2001)

\bibitem{fos:kes:2} Foster, I., Kesselman, C., Nick, J., Tuecke, S.: The Physiology of the
Grid: an Open Grid Services Architecture for Distributed Systems
Integration. Technical report, Global Grid Forum (2002)

\bibitem{url} National Center for Biotechnology Information, http://www.ncbi.nlm.nih.gov


\end{thebibliography}





\end{document}
